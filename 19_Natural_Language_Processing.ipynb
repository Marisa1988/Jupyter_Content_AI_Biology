{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be301c4-61af-4f40-be5f-57f2ba4b7733",
   "metadata": {},
   "source": [
    "# Natural Language Processing Intro\n",
    "\n",
    "> Some of the examples in this notebook are adapted from the following sources:\n",
    "> * NVIDIA DLI: [Building Transformer-Based Natural Language Processing Applications](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+C-FX-03+V3). This is an instructor-led course that I teach periodically--let me know if you have a group that would be interested in taking this workshop.\n",
    "> * LinkedIn Learning: [Introduction to Transformer Models for NLP by Sinan Ozdemir](https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/introduction). This is a great course, and like all LinkedIn Learning courses, is free to UF students, faculty and staff.\n",
    "\n",
    "\n",
    "Natural Language Processing (NLP) is a large field of AI with many related but somewhat distinct sub-disciplines. We won't have time to look at all of these, but you are most likely somewhat familiar with many of the applications.\n",
    "\n",
    "Some NLP Tasks:\n",
    "* Summary generation, information extractions\n",
    "* Translation (language to language)\n",
    "* Transcription (speech to sample_text)\n",
    "* Auto-completion\n",
    "* Sentiment Analysis\n",
    "* Intent Detection\n",
    "* Voice assistant\n",
    "* Document retrieval\n",
    "* And the BIG one these days, **Chat**, automated writing, dialog generation, question answering, etc., the kinds of things that ChatGPT is known for. \n",
    "\n",
    "## Ambiguity in language\n",
    "\n",
    "NLP is not a simple task, and until deep learning, was quite limited in its abilities. As with most fields in AI, there is a long history [dating back to the 1950s](https://en.wikipedia.org/wiki/Natural_language_processing). Part of the challenge is that human language tends to be ambiguous, and recognizing words is really only the start of inferring meaning. Take for example, this sentence:\n",
    "\n",
    "   > The boy saw a man with a telescope\n",
    "   \n",
    "   * Who had the telescope?\n",
    "   \n",
    "More context is needed to answer this. Yet, consample_text is not always there, and even when it is, it can be a challenge for NLP methods (and even human readers at times).\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "One of the challenges of NLP is representing language as numbers--remember, computers and the ML/AI systems that we have primarily deal with numbers. This was relatively easy for computer vision problems because we took the pixel intensities of an image and fed those into our models. But what should we do with speech, words, and text?\n",
    "\n",
    "The process of converting sample_text to numerical representation is called **tokenization**. There are many methods of tokenization, but the idea is to break sample_text into itemizable components--tokens.\n",
    "\n",
    "Tokens can be words, letters, word fragments, or even sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3b29b",
   "metadata": {},
   "source": [
    "## The BERT WordPiece Tokenizers\n",
    "\n",
    "From the NVIDIA Course:\n",
    "> Tokenization splits a word, phrase, or larger text section into individual characters, words, or subwords.  For example, the word \"tokenization\" could be split in a number of ways:\n",
    ">\n",
    "> * Characters: 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'\n",
    "> * Words: 'tokenization'\n",
    "> * Subwords: 'token', '##ization'\n",
    ">\n",
    "> The idea is to create a vocabulary of tokens from a sample_text corpus, which can then be trained in a language model to characterize language relationships between the tokens.  Whether this is done by character, word, or subword affects the complexity of the problem.\n",
    ">\n",
    "> Tokenization by characters has the advantage of a very limited number of tokens to deal with, but these few tokens are not very meaningful by themselves and long sequences of tokens are required to represent sample_text.  Tokenization by words results in a very large vocabulary size and requires separate tokens for very similar words, which in turn requires more training to determine their relationships to each other.\n",
    ">\n",
    "> Tokenization by subwords is a solution that tries to balance these two. For example, the word \"token\" is a subword for \"tokenization\", \"tokens\", and \"tokenize\".  By splitting the words, the model learns similar meanings from the same root word more easily.  The size of the overall vocabulary required for understanding is less than required for word tokenization.\n",
    "\n",
    "## WordPiece Algorithm\n",
    "> The WordPiece algorithm was introduced in [this paper by Schuster and Nakajima](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf).  To begin, the training data (corpus) is chosen, as well as the subword vocabulary size desired.   The algorithm iteratively determines optimal subwords for the body of sample_text and creates the vocabulary with assigned values.  The iterative steps are:\n",
    ">\n",
    "> 1. Split words into sequences of character tokens.\n",
    "> 2. Build the language model on the training data using tokens from previous step.\n",
    "> 3. Generate new unit tokens by combining two tokens with high likelihood in the language model and add the new token(s) to the vocabulary.\n",
    "> 4. Repeat from step 2 until the token limit for the desired vocabulary is reached or the likelihood falls below some desired threshold.\n",
    "\n",
    "Let's look at this in code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967ce73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/__main__.py\", line 5, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3209775/1650306133.py\", line 2, in <module>\n",
      "    from nemo.collections import nlp as nemo_nlp\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp import data, losses, models, modules\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/data/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp.data.data_utils import *\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/data/data_utils/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp.data.data_utils.data_preprocessing import *\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/data/data_utils/data_preprocessing.py\", line 25, in <module>\n",
      "    import torch\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 2183, in <module>\n",
      "    from torch import quantization as quantization  # usort: skip\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/quantization/__init__.py\", line 2, in <module>\n",
      "    from .fake_quantize import *  # noqa: F403\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/quantization/fake_quantize.py\", line 10, in <module>\n",
      "    from torch.ao.quantization.fake_quantize import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/__init__.py\", line 12, in <module>\n",
      "    from .pt2e._numeric_debugger import (  # noqa: F401\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/_numeric_debugger.py\", line 8, in <module>\n",
      "    from torch.export import ExportedProgram\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/__init__.py\", line 68, in <module>\n",
      "    from .decomp_utils import CustomDecompTable\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/decomp_utils.py\", line 5, in <module>\n",
      "    from torch._export.utils import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_export/__init__.py\", line 47, in <module>\n",
      "    from .wrappers import _wrap_submodules\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_export/wrappers.py\", line 7, in <module>\n",
      "    from torch._higher_order_ops.strict_mode import strict_mode\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_higher_order_ops/__init__.py\", line 1, in <module>\n",
      "    from torch._higher_order_ops.cond import cond\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_higher_order_ops/cond.py\", line 9, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_subclasses/functional_tensor.py\", line 45, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_subclasses/functional_tensor.py\", line 295, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      " (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/__main__.py\", line 5, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3209775/1650306133.py\", line 2, in <module>\n",
      "    from nemo.collections import nlp as nemo_nlp\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp import data, losses, models, modules\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/data/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp.data.data_utils import *\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/data/data_utils/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp.data.data_utils.data_preprocessing import *\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/data/data_utils/data_preprocessing.py\", line 28, in <module>\n",
      "    from nemo.utils import logging\n",
      "  File \"/opt/NeMo/nemo/utils/__init__.py\", line 32, in <module>\n",
      "    from nemo.utils.lightning_logger_patch import add_memory_handlers_to_pl_logger\n",
      "  File \"/opt/NeMo/nemo/utils/lightning_logger_patch.py\", line 18, in <module>\n",
      "    import lightning.pytorch as pl\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/__init__.py\", line 20, in <module>\n",
      "    from lightning.pytorch.callbacks import Callback  # noqa: E402\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/__init__.py\", line 27, in <module>\n",
      "    from lightning.pytorch.callbacks import Callback  # noqa: E402\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/__init__.py\", line 14, in <module>\n",
      "    from lightning.pytorch.callbacks.batch_size_finder import BatchSizeFinder\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/batch_size_finder.py\", line 26, in <module>\n",
      "    from lightning.pytorch.callbacks.callback import Callback\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/callback.py\", line 22, in <module>\n",
      "    from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/types.py\", line 42, in <module>\n",
      "    from torchmetrics import Metric\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torchmetrics/__init__.py\", line 37, in <module>\n",
      "    from torchmetrics import functional  # noqa: E402\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torchmetrics/functional/__init__.py\", line 125, in <module>\n",
      "    from torchmetrics.functional.text._deprecated import _bleu_score as bleu_score\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torchmetrics/functional/text/__init__.py\", line 50, in <module>\n",
      "    from torchmetrics.functional.text.bert import bert_score\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torchmetrics/functional/text/bert.py\", line 56, in <module>\n",
      "    from transformers import AutoModel, AutoTokenizer\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1806, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1805, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1817, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/modeling_auto.py\", line 21, in <module>\n",
      "    from .auto_factory import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1805, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1817, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 53, in <module>\n",
      "    from .candidate_generator import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/candidate_generator.py\", line 26, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/__init__.py\", line 15, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/_array_api.py\", line 17, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/__main__.py\", line 5, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3209775/1650306133.py\", line 2, in <module>\n",
      "    from nemo.collections import nlp as nemo_nlp\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp import data, losses, models, modules\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/data/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp.data.data_utils import *\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/data/data_utils/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp.data.data_utils.data_preprocessing import *\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/data/data_utils/data_preprocessing.py\", line 28, in <module>\n",
      "    from nemo.utils import logging\n",
      "  File \"/opt/NeMo/nemo/utils/__init__.py\", line 32, in <module>\n",
      "    from nemo.utils.lightning_logger_patch import add_memory_handlers_to_pl_logger\n",
      "  File \"/opt/NeMo/nemo/utils/lightning_logger_patch.py\", line 18, in <module>\n",
      "    import lightning.pytorch as pl\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/__init__.py\", line 20, in <module>\n",
      "    from lightning.pytorch.callbacks import Callback  # noqa: E402\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/__init__.py\", line 27, in <module>\n",
      "    from lightning.pytorch.callbacks import Callback  # noqa: E402\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/__init__.py\", line 14, in <module>\n",
      "    from lightning.pytorch.callbacks.batch_size_finder import BatchSizeFinder\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/batch_size_finder.py\", line 26, in <module>\n",
      "    from lightning.pytorch.callbacks.callback import Callback\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/callback.py\", line 22, in <module>\n",
      "    from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/types.py\", line 42, in <module>\n",
      "    from torchmetrics import Metric\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torchmetrics/__init__.py\", line 37, in <module>\n",
      "    from torchmetrics import functional  # noqa: E402\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torchmetrics/functional/__init__.py\", line 125, in <module>\n",
      "    from torchmetrics.functional.text._deprecated import _bleu_score as bleu_score\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torchmetrics/functional/text/__init__.py\", line 50, in <module>\n",
      "    from torchmetrics.functional.text.bert import bert_score\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torchmetrics/functional/text/bert.py\", line 56, in <module>\n",
      "    from transformers import AutoModel, AutoTokenizer\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1806, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1805, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1817, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/modeling_auto.py\", line 21, in <module>\n",
      "    from .auto_factory import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1805, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 1817, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 53, in <module>\n",
      "    from .candidate_generator import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/candidate_generator.py\", line 26, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/__init__.py\", line 15, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/_array_api.py\", line 17, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"/home/moralesmurallesm/.local/lib/python3.12/site-packages/sklearn/utils/fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_worker/__main__.py\", line 7, in <module>\n",
      "    from torch._inductor.async_compile import pre_fork_setup\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 2183, in <module>\n",
      "    from torch import quantization as quantization  # usort: skip\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/quantization/__init__.py\", line 2, in <module>\n",
      "    from .fake_quantize import *  # noqa: F403\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/quantization/fake_quantize.py\", line 10, in <module>\n",
      "    from torch.ao.quantization.fake_quantize import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/__init__.py\", line 12, in <module>\n",
      "    from .pt2e._numeric_debugger import (  # noqa: F401\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/_numeric_debugger.py\", line 8, in <module>\n",
      "    from torch.export import ExportedProgram\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/__init__.py\", line 68, in <module>\n",
      "    from .decomp_utils import CustomDecompTable\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/export/decomp_utils.py\", line 5, in <module>\n",
      "    from torch._export.utils import (\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_export/__init__.py\", line 47, in <module>\n",
      "    from .wrappers import _wrap_submodules\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_export/wrappers.py\", line 7, in <module>\n",
      "    from torch._higher_order_ops.strict_mode import strict_mode\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_higher_order_ops/__init__.py\", line 1, in <module>\n",
      "    from torch._higher_order_ops.cond import cond\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_higher_order_ops/cond.py\", line 9, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_subclasses/functional_tensor.py\", line 45, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_subclasses/functional_tensor.py\", line 295, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      " (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "[NeMo W 2025-03-26 11:11:46 nemo_logging:405] /opt/megatron-lm/megatron/core/transformer/cuda_graphs.py:741: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "      assert (\n",
      "    \n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/__main__.py\", line 5, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3209775/1650306133.py\", line 2, in <module>\n",
      "    from nemo.collections import nlp as nemo_nlp\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp import data, losses, models, modules\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/models/__init__.py\", line 16, in <module>\n",
      "    from nemo.collections.nlp.models.duplex_text_normalization import (\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/models/duplex_text_normalization/__init__.py\", line 15, in <module>\n",
      "    from nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder import DuplexDecoderModel\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py\", line 34, in <module>\n",
      "    from nemo.collections.nlp.models.nlp_model import NLPModel\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/models/nlp_model.py\", line 40, in <module>\n",
      "    from nemo.collections.nlp.parts.nlp_overrides import NLPSaveRestoreConnector\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 128, in <module>\n",
      "    from modelopt.torch.opt.plugins import restore_sharded_modelopt_state, save_sharded_modelopt_state\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/modelopt/torch/__init__.py\", line 24, in <module>\n",
      "    from . import distill, nas, opt, prune, quantization, sparsity, speculative, utils  # noqa: E402\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/modelopt/torch/prune/__init__.py\", line 25, in <module>\n",
      "    from . import config, mode, plugins\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/modelopt/torch/prune/mode.py\", line 42, in <module>\n",
      "    from .mcore_gpt_minitron import MCoreGPTMinitronSearcher\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/modelopt/torch/prune/mcore_gpt_minitron.py\", line 49, in <module>\n",
      "    from nemo.collections import llm\n",
      "  File \"/opt/NeMo/nemo/collections/llm/__init__.py\", line 21, in <module>\n",
      "    from nemo.collections.llm.bert.data import BERTMockDataModule, BERTPreTrainingDataModule, SpecterDataModule\n",
      "  File \"/opt/NeMo/nemo/collections/llm/bert/data/__init__.py\", line 3, in <module>\n",
      "    from nemo.collections.llm.bert.data.specter import SpecterDataModule\n",
      "  File \"/opt/NeMo/nemo/collections/llm/bert/data/specter.py\", line 18, in <module>\n",
      "    from datasets import DatasetDict, load_dataset\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\", line 17, in <module>\n",
      "    from .arrow_dataset import Dataset\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 62, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    }
   ],
   "source": [
    "# Import nemo nlp collection \n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# Import BERT\n",
    "from nemo.collections.nlp.models import BERTLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98970194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PretrainedModelInfo(\n",
       " \tpretrained_model_name=bertbaseuncased,\n",
       " \tdescription=The model was trained EN Wikipedia and BookCorpus on a sequence length of 512.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/bertbaseuncased/versions/1.0.0rc1/files/bertbaseuncased.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=bertlargeuncased,\n",
       " \tdescription=The model was trained EN Wikipedia and BookCorpus on a sequence length of 512.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/bertlargeuncased/versions/1.0.0rc1/files/bertlargeuncased.nemo\n",
       " )]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the list of pre-trained BERT language models\n",
    "BERTLMModel.list_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b73d1d-fd5a-4cc5-b60e-2cbf0177502b",
   "metadata": {},
   "source": [
    "> There are two pretrained BERT language models available with NeMo: \n",
    "> - `bertbaseuncased` model has 110 millions parameters in total with 12 Transformer blocks.\n",
    "> - `bertlargeuncased` model has 340 millions parameters in total with 24 Transformer blocks.\n",
    "> \n",
    "> For the sake of time and simplicity, we'll download the smaller variant, BERT Base. This could take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a6839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-26 11:14:23 nemo_logging:393] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/bertbaseuncased/versions/1.0.0rc1/files/bertbaseuncased.nemo to /home/moralesmurallesm/.cache/torch/NeMo/NeMo_2.2.0rc4/bertbaseuncased/a88245d85161094369a11077be3e99be/bertbaseuncased.nemo\n",
      "[NeMo I 2025-03-26 11:14:32 nemo_logging:393] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-26 11:14:42 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    data_file: /home/yzhang/data/nlp/bert/47316/hdf5/lower_case_1_seq_len_512_max_pred_80_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus/training/\n",
      "    max_predictions_per_seq: 80\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 2\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2025-03-26 11:14:42 nemo_logging:405] bert-base-uncased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n",
      "[NeMo W 2025-03-26 11:14:42 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    data_file: /home/yzhang/data/nlp/bert/47316/hdf5/lower_case_1_seq_len_512_max_pred_80_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus/training/\n",
      "    max_predictions_per_seq: 80\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 2\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2025-03-26 11:14:42 nemo_logging:405] bert-base-uncased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n",
      "[NeMo W 2025-03-26 11:14:47 nemo_logging:405] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-26 11:14:47 nemo_logging:393] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        capturable: False\n",
      "        differentiable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        fused: None\n",
      "        lr: 4.375e-05\n",
      "        maximize: False\n",
      "        weight_decay: 0.01\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-26 11:14:47 nemo_logging:405] Neither `max_steps` nor `iters_per_batch` were provided to `optim.sched`, cannot compute effective `max_steps` !\n",
      "    Scheduler will not be instantiated !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-26 11:14:48 nemo_logging:393] Model BERTLMModel was successfully restored from /home/moralesmurallesm/.cache/torch/NeMo/NeMo_2.2.0rc4/bertbaseuncased/a88245d85161094369a11077be3e99be/bertbaseuncased.nemo.\n"
     ]
    }
   ],
   "source": [
    "# Download the pretrained BERT-based model\n",
    "pretrained_model_name=\"bertbaseuncased\"\n",
    "model = BERTLMModel.from_pretrained(pretrained_model_name, strict=False) # Pass strict=False to avoid needing to specify\n",
    "                                                                        # training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3910c74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights in bertbaseuncased: 110697020\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the number of weights in the model\n",
    "print(f'Number of weights in {pretrained_model_name}: {model.num_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb897d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencepiece',\n",
       " 'char',\n",
       " 'word',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'google-bert/bert-base-cased-finetuned-mrpc',\n",
       " 'openai-community/gpt2-large',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'almanach/camembert-base',\n",
       " 'FacebookAI/roberta-large',\n",
       " 'albert/albert-xxlarge-v2',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'openai-community/gpt2-xl',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'albert/albert-large-v1',\n",
       " 'FacebookAI/roberta-base',\n",
       " 'FacebookAI/roberta-large-mnli',\n",
       " 'google-bert/bert-large-uncased',\n",
       " 'distilbert-base-german-cased',\n",
       " 'google-bert/bert-large-cased',\n",
       " 'albert/albert-xlarge-v2',\n",
       " 'distilbert/distilgpt2',\n",
       " 'google-bert/bert-base-uncased',\n",
       " 'google-bert/bert-base-multilingual-cased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'openai-community/roberta-large-openai-detector',\n",
       " 'google-bert/bert-large-uncased-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'google-bert/bert-base-german-dbmdz-cased',\n",
       " 'openai-community/roberta-base-openai-detector',\n",
       " 'albert/albert-xxlarge-v1',\n",
       " 'albert/albert-base-v2',\n",
       " 'google-bert/bert-base-german-dbmdz-uncased',\n",
       " 'google-bert/bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'distilbert-base-uncased',\n",
       " 'google-bert/bert-large-cased-whole-word-masking',\n",
       " 'albert/albert-xlarge-v1',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'google-bert/bert-base-chinese',\n",
       " 'google-bert/bert-base-multilingual-uncased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'openai-community/gpt2',\n",
       " 'distilbert-base-cased',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'google-bert/bert-base-cased',\n",
       " 'distilbert/distilroberta-base',\n",
       " 'Musixmatch/umberto-commoncrawl-cased-v1',\n",
       " 'google-bert/bert-base-german-cased',\n",
       " 'albert/albert-base-v1',\n",
       " 'google-bert/bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'openai-community/gpt2-medium',\n",
       " 'albert/albert-large-v2',\n",
       " 'Musixmatch/umberto-wikipedia-uncased-v1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available tokenizers\n",
    "nemo_nlp.modules.get_tokenizer_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd78347-fd30-41a5-808a-b1da7dc1ba95",
   "metadata": {},
   "source": [
    "As I indicated, there are **a lot** of options on how to do tokenization...though in general, you will need to use what was used in training the model if you use a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11732637-6fa3-4a62-9e1f-1aaa07155d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-26 11:19:30 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: None, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    }
   ],
   "source": [
    "# Get the bert-base-uncased tokenizer \n",
    "tokenizer_uncased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfcc4b5e-bd4a-4c08-87cf-0472b9a64264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size of bertbaseuncased: 30522\n"
     ]
    }
   ],
   "source": [
    "# Check the vocabulary size\n",
    "print(f'The vocabulary size of {pretrained_model_name}: {tokenizer_uncased.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69722feb-7d4b-4bf0-8ce5-fe3422f75500",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sample_text = \"Hello, my name is Matt. I live in Gainesville, FL.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ab212db-8bc2-4d44-9a71-59a6cd203fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Hello, my name is Matt. I live in Gainesville, FL.\n",
      "Tokenized sentence: ['hello', ',', 'my', 'name', 'is', 'matt', '.', 'i', 'live', 'in', 'gaines', '##ville', ',', 'fl', '.']\n"
     ]
    }
   ],
   "source": [
    "output_uncased=tokenizer_uncased.text_to_tokens(sample_sample_text)\n",
    "print(f'Input sentence: {sample_sample_text}')\n",
    "print(f'Tokenized sentence: {output_uncased}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51da8e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-26 11:21:52 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-cased, vocab_file: None, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    }
   ],
   "source": [
    "# Get the bert-base-cased tokenizer \n",
    "tokenizer_cased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22565c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Hello, my name is Matt. I live in Gainesville, FL.\n",
      "Tokenized sentence: ['Hello', ',', 'my', 'name', 'is', 'Matt', '.', 'I', 'live', 'in', 'G', '##aines', '##ville', ',', 'FL', '.']\n"
     ]
    }
   ],
   "source": [
    "# Encode the sample_text \n",
    "output_cased=tokenizer_cased.text_to_tokens(sample_sample_text)\n",
    "print(f'Input sentence: {sample_sample_text}')\n",
    "print(f'Tokenized sentence: {output_cased}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a91ea-d186-4ef7-baef-551508d0d332",
   "metadata": {},
   "source": [
    "> The BERT model [or any NLP model really] does not accept sample_text inputs, but rather their numerical index representations.\n",
    ">\n",
    "> We can check the vocabulary index of a word using the `tokenizer.sample_text_to_ids()` function. \n",
    ">\n",
    "> Try it with the `bert-base-cased` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60e2357b-00a8-4ad6-aa4d-759febfaebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of Hello: [8667]\n",
      "Index of hello: [19082]\n"
     ]
    }
   ],
   "source": [
    "# Index of the tokens Hello and hello using bert-base-cased tokenizer\n",
    "print(f'Index of Hello: {tokenizer_cased.text_to_ids(\"Hello\")}')\n",
    "print(f'Index of hello: {tokenizer_cased.text_to_ids(\"hello\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39755ed2-2e05-4a72-98a3-0b947e3ce095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Hello, my name is Matt. I live in Gainesville, FL.\n",
      "Tokenized sentence: ['Hello', ',', 'my', 'name', 'is', 'Matt', '.', 'I', 'live', 'in', 'G', '##aines', '##ville', ',', 'FL', '.']\n",
      "Tokenized sentence: [8667, 117, 1139, 1271, 1110, 3895, 119, 146, 1686, 1107, 144, 25180, 2138, 117, 23485, 119]\n"
     ]
    }
   ],
   "source": [
    "# Example of bert-base-cased tokenizer in a sentence\n",
    "print(f'Input sentence: {sample_sample_text}')\n",
    "print(f'Tokenized sentence: {output_cased}')\n",
    "print(f'Tokenized sentence: {tokenizer_cased.text_to_ids(sample_sample_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef736ba-456f-4094-9229-b09eb77dba47",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Embedding\n",
    "\n",
    "The tokenization process takes care of converting words to numbers. This is a first step, but we want to contextualize the words--i.e., what does each word mean? Embedding goes a step further, representing words in n-dimensional vector space. This vector space has a lower dimensionality than the number of words in the vocabulary (as opposed to one-hot encoding) and results in words with similar meaning being closer in space than other words.\n",
    "\n",
    "Here's an example figure showing some embeddings (taken from Renu Khandelwal's article [*Word Embeddings for NLP*](https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4):\n",
    "\n",
    "\n",
    "![Image of word embeddings from https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4 ](images/word_embeddings_Renu_Khandelwal.png)\n",
    "\n",
    "The embedding is learned in the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b32991b-10bc-491f-8fa9-8b799b271511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up the sentence we want to look at\n",
    "sample_text = \"Last night, my wireless mouse was eaten by an animal such as a mouse or rat. I need to order a new optical computer mouse.\"\n",
    "input_sentence=torch.tensor([tokenizer_uncased.tokenizer(sample_text).input_ids]).cuda()\n",
    "attention_mask=torch.tensor([tokenizer_uncased.tokenizer(sample_text).attention_mask]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4c2d68b-a554-42f8-83e9-e5d413f19849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Last night, my wireless mouse was eaten by an animal such as a mouse or rat. I need to order a new optical computer mouse.\n",
      "Tokenized sentence: ['[CLS]', 'last', 'night', ',', 'my', 'wireless', 'mouse', 'was', 'eaten', 'by', 'an', 'animal', 'such', 'as', 'a', 'mouse', 'or', 'rat', '.', 'i', 'need', 'to', 'order', 'a', 'new', 'optical', 'computer', 'mouse', '.', '[SEP]']\n",
      "Check that numbers line up:\n",
      "  Should be \"mouse, mouse, mouse\": mouse, mouse, mouse\n"
     ]
    }
   ],
   "source": [
    "# Show the tokenization for the sentence\n",
    "print(f'Input sentence: {sample_text}')\n",
    "output_uncased=tokenizer_uncased.ids_to_tokens(tokenizer_uncased.tokenizer(sample_text).input_ids)\n",
    "print(f'Tokenized sentence: {output_uncased}')\n",
    "\n",
    "# \"mouse\" tokens positions in the sample_text input\n",
    "mouse_computer_1=6\n",
    "mouse_animal=15\n",
    "mouse_computer_2=27\n",
    "\n",
    "print(\"Check that numbers line up:\")\n",
    "print(f'  Should be \"mouse, mouse, mouse\": {output_uncased[mouse_computer_1]}, {output_uncased[mouse_animal]}, {output_uncased[mouse_computer_2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67c6f794-80b6-4211-ab40-6153aefd8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings for the pretrained model\n",
    "hidden_states = model.bert_model(input_ids=input_sentence, token_type_ids=None, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59548dc7-1f8a-410d-bacc-796fcff53693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def similarity_cosine(x,y):\n",
    "    return dot(x,y)/(norm(x)*norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98537425-4c28-4ade-8f6b-9cbcc3991f50",
   "metadata": {},
   "source": [
    "> We can visualize sample_text token embeddings obtained from the BERT models if we first reduce the dimensionality to 2D. \n",
    "[t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) is a dimensionality reduction technique widely used for vector visualization in 2D or 3D, as it preserves the neighborhood distances.\n",
    ">\n",
    "> The following codeblocks uses the [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) implementation of the t-SNE algorithm to reduce the dimensionality of BERT sample_text token embeddings from 768 to 2, and plots the 2D vectors. Note that as t-SNE is a stochastic process, the low dimensional embeddings will vary from one run to another. However, the neighborhood distances of tokens should remain more or less the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7bb84c7-3e81-4e86-9e85-b326fe1d8ae0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X\u001b[38;5;241m=\u001b[39m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m X_embedded \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m,  init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(X[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m Tokens\u001b[38;5;241m=\u001b[39mtokenizer_uncased\u001b[38;5;241m.\u001b[39mids_to_tokens(tokenizer_uncased\u001b[38;5;241m.\u001b[39mtokenizer(sample_text)\u001b[38;5;241m.\u001b[39minput_ids)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "X=hidden_states.cpu().detach().numpy()\n",
    "X_embedded = TSNE(n_components=2,metric='euclidean',  init='random', perplexity=7).fit_transform(X[0])\n",
    "Tokens=tokenizer_uncased.ids_to_tokens(tokenizer_uncased.tokenizer(sample_text).input_ids)\n",
    "\n",
    "# Annotate the different mouse tokens\n",
    "Tokens[mouse_computer_1]=\"mouse_computer_1\"\n",
    "Tokens[mouse_animal]=\"mouse_animal\"\n",
    "Tokens[mouse_computer_2]=\"mouse_computer_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc837b6f-99af-4fc7-bbb1-5eb9c9a3c7a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_embedded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mX_embedded\u001b[49m[:,\u001b[38;5;241m0\u001b[39m],X_embedded[:,\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([X_embedded[mouse_computer_1,\u001b[38;5;241m0\u001b[39m],X_embedded[mouse_animal,\u001b[38;5;241m0\u001b[39m]],[X_embedded[mouse_computer_1,\u001b[38;5;241m1\u001b[39m],X_embedded[mouse_animal,\u001b[38;5;241m1\u001b[39m]],color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([X_embedded[mouse_computer_1,\u001b[38;5;241m0\u001b[39m],X_embedded[mouse_computer_2,\u001b[38;5;241m0\u001b[39m]],[X_embedded[mouse_computer_1,\u001b[38;5;241m1\u001b[39m],X_embedded[mouse_computer_2,\u001b[38;5;241m1\u001b[39m]],color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_embedded' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(X_embedded[:,0],X_embedded[:,1], '.', color='black')\n",
    "plt.plot([X_embedded[mouse_computer_1,0],X_embedded[mouse_animal,0]],[X_embedded[mouse_computer_1,1],X_embedded[mouse_animal,1]],color='red')\n",
    "plt.plot([X_embedded[mouse_computer_1,0],X_embedded[mouse_computer_2,0]],[X_embedded[mouse_computer_1,1],X_embedded[mouse_computer_2,1]],color='green')\n",
    "\n",
    "for i, txt in enumerate(Tokens):\n",
    "    plt.annotate(txt, (X_embedded[i,0], X_embedded[i,1]), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813a598-e50b-4f9b-a2e1-d10f85c6efdd",
   "metadata": {},
   "source": [
    "## Tokenizers and Embeddings are learned\n",
    "\n",
    "> The tokenizer algorithm generates the vocabulary following variants of Top-K frequent words from the text corpus.\n",
    ">\n",
    "> The vocabulary size is limited because the training cost increases with the size of the vocabulary. Including all unique words from the text corpus into the vocabulary would explode the complexity of training beyond the capabilities of the tokenizer. For instance, the BERT model that was released in 2018, with a subword tokenizer algorithm called WordPiece, has a vocabulary limit of 30,000.\n",
    ">\n",
    "> How, then, do tokenizers deal with terms that are not part of the vocabulary or **out-of-vocabulary (OOV)** words?\n",
    ">\n",
    "> 1. One option is to replace OOV words with a special token, `[UNK].` In this case, all OOV terms will have the same representation for the neural network losing the semantic. \n",
    "> 1. A second option is to split OOV words at the character level. This increases the size of the input to the neural language model, adding the challenge of learning the relationship between characters to keep the semantic.\n",
    "> 1. Sub-word tokenizers, such as BERT WordPiece, provide a solution in between the word token and character split option. It tokenizes OOV words into subwords.\n",
    ">\n",
    "> Let's have a look at the `bert-base-uncased` tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02da9559-ac16-4b97-8a05-8d41f8095b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-26 11:32:41 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: None, merges_files: None, special_tokens_dict: {}, and use_fast: False\n",
      "The vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# load the bert-base-uncased tokenizer \n",
    "tokenizer_uncased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-uncased\")\n",
    "\n",
    "# get the vocabulary size\n",
    "print(f\"The vocabulary size: {tokenizer_uncased.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379973df-a77a-4914-8ba2-4f09a2b61324",
   "metadata": {},
   "source": [
    "> As an example, take a look at the format tokenization for years with BERT. Years prior to 2021 appear frequently enough in the corpus to be part of the vocabulary, while years in the future are OOV and are split into sub-tokens.\n",
    ">\n",
    "> Try it in the cell below using the `tokenizer_uncased.text_to_tokens()` function for various years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0448c4b-4213-46a6-a77e-5d4d5e014660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized year: ['2019']\n",
      "Tokenized year: ['2020']\n",
      "Tokenized year: ['2021']\n",
      "Tokenized year: ['202', '##2']\n",
      "Tokenized year: ['202', '##3']\n",
      "Tokenized year: ['203', '##0']\n"
     ]
    }
   ],
   "source": [
    "# Bert tokenizer for years\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2019')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2020')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2021')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2022')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2023')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2030')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c2e7b-bac8-494c-82f8-b1b3a8e3df3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeMo NLP",
   "language": "python",
   "name": "nemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
